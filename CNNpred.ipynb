{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNpred.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raghav2069/Stock-market-prediction-using-CNN/blob/main/CNNpred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3d CNN"
      ],
      "metadata": {
        "id": "aetBp2LosLo5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrw5VxCysJUs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import scale\n",
        "from os.path import join\n",
        "from sklearn.metrics import accuracy_score as accuracy, f1_score, mean_absolute_error as mae\n",
        "import os\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
        "from pathlib2 import Path\n",
        "from tensorflow.keras import backend as K, callbacks\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "\n",
        "        Only computes a batch-wise average of precision.\n",
        "\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision_pos = precision(y_true, y_pred)\n",
        "    recall_pos = recall(y_true, y_pred)\n",
        "    precision_neg = precision((K.ones_like(y_true)-y_true), (K.ones_like(y_pred)-K.clip(y_pred, 0, 1)))\n",
        "    recall_neg = recall((K.ones_like(y_true)-y_true), (K.ones_like(y_pred)-K.clip(y_pred, 0, 1)))\n",
        "    f_posit = 2*((precision_pos*recall_pos)/(precision_pos+recall_pos+K.epsilon()))\n",
        "    f_neg = 2 * ((precision_neg * recall_neg) / (precision_neg + recall_neg + K.epsilon()))\n",
        "\n",
        "    return (f_posit + f_neg) / 2\n",
        "\n",
        "def load_data(file_fir):\n",
        "    try:\n",
        "        df_raw = pd.read_csv(file_fir, parse_dates=['Date'])\n",
        "        df_raw.index = df_raw['Date']\n",
        "    except IOError:\n",
        "        print(\"IO ERROR\")\n",
        "    return df_raw\n",
        "\n",
        "def construct_data_warehouse(ROOT_PATH, file_names):\n",
        "    global number_of_stocks\n",
        "    global samples_in_each_stock\n",
        "    global number_feature\n",
        "    global predict_index\n",
        "    global order_stocks\n",
        "    tottal_train_data = np.empty((0,82))\n",
        "    tottal_train_target = np.empty((0))\n",
        "    tottal_test_data = np.empty((0,82))\n",
        "    tottal_test_target = np.empty((0))\n",
        "\n",
        "    for stock_file_name in file_names:\n",
        "\n",
        "        file_dir = os.path.join(ROOT_PATH, stock_file_name)\n",
        "        ## Loading Data\n",
        "        try:\n",
        "            df_raw = load_data(file_dir)\n",
        "        except ValueError:\n",
        "            print(\"Couldn't Read {} file\".format(file_dir))\n",
        "\n",
        "        number_of_stocks += 1\n",
        "\n",
        "        data = df_raw\n",
        "        df_name = data['Name'][0]\n",
        "        order_stocks.append(df_name)\n",
        "        del data['Name']\n",
        "\n",
        "        target = (data['Close'][predict_day:] / data['Close'][:-predict_day].values).astype(int)\n",
        "        data = data[:-predict_day]\n",
        "        target.index = data.index\n",
        "        # Becasue of using 200 days Moving Average as one of the features\n",
        "        data = data[200:]\n",
        "        data = data.fillna(0)\n",
        "        data['target'] = target\n",
        "        target = data['target']\n",
        "        del data['target']\n",
        "        del data['Date']\n",
        "        # data['Date'] = data['Date'].apply(lambda x: x.weekday())\n",
        "\n",
        "        number_feature = data.shape[1]\n",
        "        samples_in_each_stock = data.shape[0]\n",
        "\n",
        "        train_data = data[data.index < '2016-04-21']\n",
        "        train_data = scale(train_data)\n",
        "\n",
        "        if df_name == predict_index:\n",
        "            tottal_train_target = target[target.index < '2016-04-21']\n",
        "            tottal_test_target = target[target.index >= '2016-04-21']\n",
        "\n",
        "        data = pd.DataFrame(scale(data.values), columns=data.columns)\n",
        "        data.index = target.index\n",
        "        test_data = data[data.index >= '2016-04-21']\n",
        "\n",
        "        tottal_train_data = np.concatenate((tottal_train_data, train_data))\n",
        "        tottal_test_data = np.concatenate((tottal_test_data, test_data))\n",
        "\n",
        "    train_size = int(tottal_train_data.shape[0]/number_of_stocks)\n",
        "    test_size = int(tottal_test_data.shape[0] / number_of_stocks)\n",
        "    tottal_train_data = tottal_train_data.reshape(number_of_stocks, train_size, number_feature)\n",
        "    tottal_test_data = tottal_test_data.reshape(number_of_stocks, test_size, number_feature)\n",
        "\n",
        "\n",
        "    return tottal_train_data, tottal_test_data, tottal_train_target, tottal_test_target\n",
        "\n",
        "def cnn_data_sequence(data, target, seque_len):\n",
        "    print ('sequencing data ...')\n",
        "    new_train = []\n",
        "    new_target = []\n",
        "\n",
        "    for index in range(data.shape[1] - seque_len + 1):\n",
        "        new_train.append(data[:, index: index + seque_len])\n",
        "        new_target.append(target[index + seque_len - 1])\n",
        "\n",
        "    new_train = np.array(new_train)\n",
        "    new_target = np.array(new_target)\n",
        "\n",
        "    return new_train, new_target\n",
        "\n",
        "def sklearn_acc(model, test_data, test_target):\n",
        "    overall_results = model.predict(test_data)\n",
        "    test_pred = (overall_results > 0.5).astype(int)\n",
        "    acc_results = [mae(overall_results, test_target), accuracy(test_pred, test_target),\n",
        "                   f1_score(test_pred, test_target, average='macro')]\n",
        "\n",
        "    return acc_results\n",
        "\n",
        "def CNN(train_data, test_data, train_target, test_target):\n",
        "    # hisory of data in each sample\n",
        "    seq_len = 60\n",
        "    epoc = 100\n",
        "    drop = 0.1\n",
        "\n",
        "    # creating sample each containing #seq_len history\n",
        "    cnn_train_data, cnn_train_target = cnn_data_sequence(train_data, train_target, seq_len)\n",
        "    cnn_test_data, cnn_test_target = cnn_data_sequence(test_data, test_target, seq_len)\n",
        "    result = []\n",
        "\n",
        "    # Running CNNpred several times\n",
        "    for i in range(1,40):\n",
        "        K.clear_session()\n",
        "        print ('i: ', i)\n",
        "        my_file = Path( join(Base_dir, '3D-models/{}/model/{}-{}-{}-{}-{}.h5'.format(predict_index, epoc, seq_len, number_filter, drop, i)))\n",
        "        filepath = join(Base_dir, '3D-models/{}/model/{}-{}-{}-{}-{}.h5'.format(predict_index, epoc, seq_len, number_filter, drop, i))\n",
        "\n",
        "        # If the trained model doesn't exit, it is trained\n",
        "        if my_file.is_file():\n",
        "            print('loading model')\n",
        "\n",
        "        else:\n",
        "            print('fitting model')\n",
        "            model = Sequential()\n",
        "\n",
        "            #layer 1\n",
        "            model.add(Conv2D(number_filter[0], (1, 1), activation='relu', input_shape=(number_of_stocks,seq_len, number_feature), data_format='channels_last'))\n",
        "            #layer 2\n",
        "            model.add(Conv2D(number_filter[1], (number_of_stocks, 3), activation='relu'))\n",
        "            model.add(MaxPool2D(pool_size=(1, 2)))\n",
        "\n",
        "            #layer 3\n",
        "            model.add(Conv2D(number_filter[2], (1, 3), activation='relu'))\n",
        "            model.add(MaxPool2D(pool_size=(1, 2)))\n",
        "\n",
        "            model.add(Flatten())\n",
        "            model.add(Dropout(drop))\n",
        "            model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "            model.compile(optimizer='Adam', loss='mae', metrics=['acc',f1])\n",
        "\n",
        "            best_model = callbacks.ModelCheckpoint(filepath, monitor='val_f1', verbose=0, save_best_only=True,\n",
        "                                                   save_weights_only=False, mode='max', period=1)\n",
        "\n",
        "            model.fit(cnn_train_data, cnn_train_target, epochs=epoc, batch_size=128, verbose=0,callbacks=[best_model], validation_split=0.25)\n",
        "\n",
        "        model = load_model(filepath, custom_objects={'f1': f1})\n",
        "        test_pred = sklearn_acc(model,cnn_test_data, cnn_test_target)\n",
        "        print (test_pred)\n",
        "        result.append(test_pred)\n",
        "\n",
        "    print('saving results')\n",
        "    results = pd.DataFrame(result , columns=['MAE', 'Accuracy', 'F-score'])\n",
        "    results = results.append([results.mean(), results.max(), results.std()], ignore_index=True)\n",
        "    results.to_csv(join(Base_dir, '3D-models/{}/new results.csv'.format(predict_index)), index=False)\n",
        "\n",
        "\n",
        "Base_dir = ''\n",
        "TRAIN_ROOT_PATH = join(Base_dir, 'Dataset')\n",
        "train_file_names = os.listdir(join(Base_dir, 'Dataset'))\n",
        "\n",
        "# if moving average = 0 then we have no moving average\n",
        "moving_average_day = 0\n",
        "number_of_stocks = 0\n",
        "number_feature = 0\n",
        "samples_in_each_stock = 0\n",
        "number_filter = [8,8,8]\n",
        "predict_day = 1\n",
        "order_stocks = []\n",
        "# Name of the index that is going to be predicted\n",
        "predict_index = 'DJI'   # RUT, S&P, NYA, NASDAQ, DJI\n",
        "\n",
        "\n",
        "print ('Loading train data ...')\n",
        "train_data, test_data, train_target, test_target = construct_data_warehouse(TRAIN_ROOT_PATH, train_file_names)\n",
        "print ('number of stocks = ', number_of_stocks)\n",
        "print ('fitting model')\n",
        "\n",
        "CNN(train_data, test_data, train_target, test_target)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2d CNN"
      ],
      "metadata": {
        "id": "AexKdpzmscV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import scale\n",
        "from os.path import join\n",
        "from sklearn.metrics import accuracy_score as accuracy, f1_score, mean_absolute_error as mae\n",
        "import os\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
        "from pathlib2 import Path\n",
        "from tensorflow.keras import backend as K, callbacks\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "\n",
        "        Only computes a batch-wise average of precision.\n",
        "\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "    precision_pos = precision(y_true, y_pred)\n",
        "    recall_pos = recall(y_true, y_pred)\n",
        "    precision_neg = precision((K.ones_like(y_true) - y_true), (K.ones_like(y_pred) - K.clip(y_pred, 0, 1)))\n",
        "    recall_neg = recall((K.ones_like(y_true) - y_true), (K.ones_like(y_pred) - K.clip(y_pred, 0, 1)))\n",
        "    f_posit = 2 * ((precision_pos * recall_pos) / (precision_pos + recall_pos + K.epsilon()))\n",
        "    f_neg = 2 * ((precision_neg * recall_neg) / (precision_neg + recall_neg + K.epsilon()))\n",
        "\n",
        "    return (f_posit + f_neg) / 2\n",
        "\n",
        "\n",
        "def load_data(file_fir):\n",
        "    try:\n",
        "        df_raw = pd.read_csv(file_fir, index_col='Date') # parse_dates=['Date'])\n",
        "    except IOError:\n",
        "        print(\"IO ERROR\")\n",
        "    return df_raw\n",
        "\n",
        "\n",
        "def costruct_data_warehouse(ROOT_PATH, file_names):\n",
        "    global number_of_stocks\n",
        "    global samples_in_each_stock\n",
        "    global number_feature\n",
        "    global order_stocks\n",
        "    data_warehouse = {}\n",
        "\n",
        "    for stock_file_name in file_names:\n",
        "\n",
        "        file_dir = os.path.join(ROOT_PATH, stock_file_name)\n",
        "        ## Loading Data\n",
        "        try:\n",
        "            df_raw = load_data(file_dir)\n",
        "        except ValueError:\n",
        "            print(\"Couldn't Read {} file\".format(file_dir))\n",
        "\n",
        "        number_of_stocks += 1\n",
        "\n",
        "        data = df_raw\n",
        "        df_name = data['Name'][0]\n",
        "        order_stocks.append(df_name)\n",
        "        del data['Name']\n",
        "\n",
        "        target = (data['Close'][predict_day:] / data['Close'][:-predict_day].values).astype(int)\n",
        "        data = data[:-predict_day]\n",
        "        target.index = data.index\n",
        "        # Becasue of using 200 days Moving Average as one of the features\n",
        "        data = data[200:]\n",
        "        data = data.fillna(0)\n",
        "        data['target'] = target\n",
        "        target = data['target']\n",
        "        # data['Date'] = data['Date'].apply(lambda x: x.weekday())\n",
        "        del data['target']\n",
        "\n",
        "        number_feature = data.shape[1]\n",
        "        samples_in_each_stock = data.shape[0]\n",
        "\n",
        "        train_data = data[data.index < '2016-04-21']\n",
        "        train_data1 = scale(train_data)\n",
        "        # print train_data.shape\n",
        "        train_target1 = target[target.index < '2016-04-21']\n",
        "        train_data = train_data1[:int(0.75 * train_data1.shape[0])]\n",
        "        train_target = train_target1[:int(0.75 * train_target1.shape[0])]\n",
        "\n",
        "        valid_data = scale(train_data1[int(0.75 * train_data1.shape[0]) - seq_len:])\n",
        "        valid_target = train_target1[int(0.75 * train_target1.shape[0]) - seq_len:]\n",
        "\n",
        "        data = pd.DataFrame(scale(data.values), columns=data.columns)\n",
        "        data.index = target.index\n",
        "        test_data = data[data.index >= '2016-04-21']\n",
        "        test_target = target[target.index >= '2016-04-21']\n",
        "\n",
        "        data_warehouse[df_name] = [train_data, train_target, np.array(test_data), np.array(test_target), valid_data,\n",
        "                                   valid_target]\n",
        "\n",
        "    return data_warehouse\n",
        "\n",
        "\n",
        "def cnn_data_sequence_separately(tottal_data, tottal_target, data, target, seque_len):\n",
        "    for index in range(data.shape[0] - seque_len + 1):\n",
        "        tottal_data.append(data[index: index + seque_len])\n",
        "        tottal_target.append(target[index + seque_len - 1])\n",
        "\n",
        "    return tottal_data, tottal_target\n",
        "\n",
        "\n",
        "def cnn_data_sequence(data_warehouse, seq_len):\n",
        "    tottal_train_data = []\n",
        "    tottal_train_target = []\n",
        "    tottal_valid_data = []\n",
        "    tottal_valid_target = []\n",
        "    tottal_test_data = []\n",
        "    tottal_test_target = []\n",
        "\n",
        "    for key, value in data_warehouse.items():\n",
        "        tottal_train_data, tottal_train_target = cnn_data_sequence_separately(tottal_train_data, tottal_train_target,\n",
        "                                                                              value[0], value[1], seq_len)\n",
        "        tottal_test_data, tottal_test_target = cnn_data_sequence_separately(tottal_test_data, tottal_test_target,\n",
        "                                                                            value[2], value[3], seq_len)\n",
        "        tottal_valid_data, tottal_valid_target = cnn_data_sequence_separately(tottal_valid_data, tottal_valid_target,\n",
        "                                                                              value[4], value[5], seq_len)\n",
        "\n",
        "    tottal_train_data = np.array(tottal_train_data)\n",
        "    tottal_train_target = np.array(tottal_train_target)\n",
        "    tottal_test_data = np.array(tottal_test_data)\n",
        "    tottal_test_target = np.array(tottal_test_target)\n",
        "    tottal_valid_data = np.array(tottal_valid_data)\n",
        "    tottal_valid_target = np.array(tottal_valid_target)\n",
        "\n",
        "    tottal_train_data = tottal_train_data.reshape(tottal_train_data.shape[0], tottal_train_data.shape[1],\n",
        "                                                  tottal_train_data.shape[2], 1)\n",
        "    tottal_test_data = tottal_test_data.reshape(tottal_test_data.shape[0], tottal_test_data.shape[1],\n",
        "                                                tottal_test_data.shape[2], 1)\n",
        "    tottal_valid_data = tottal_valid_data.reshape(tottal_valid_data.shape[0], tottal_valid_data.shape[1],\n",
        "                                                  tottal_valid_data.shape[2], 1)\n",
        "\n",
        "    return tottal_train_data, tottal_train_target, tottal_test_data, tottal_test_target, tottal_valid_data, tottal_valid_target\n",
        "\n",
        "\n",
        "def sklearn_acc(model, test_data, test_target):\n",
        "    overall_results = model.predict(test_data)\n",
        "    test_pred = (overall_results > 0.5).astype(int)\n",
        "    acc_results = [mae(overall_results, test_target), accuracy(test_pred, test_target),\n",
        "                   f1_score(test_pred, test_target, average='macro')]\n",
        "\n",
        "    return acc_results\n",
        "\n",
        "\n",
        "def train(data_warehouse, i):\n",
        "    seq_len = 60\n",
        "    epochs = 200\n",
        "    drop = 0.1\n",
        "\n",
        "    global cnn_train_data, cnn_train_target, cnn_test_data, cnn_test_target, cnn_valid_data, cnn_valid_target\n",
        "\n",
        "    if i == 1:\n",
        "        print('sequencing ...')\n",
        "        cnn_train_data, cnn_train_target, cnn_test_data, cnn_test_target, cnn_valid_data, cnn_valid_target = cnn_data_sequence(\n",
        "            data_warehouse, seq_len)\n",
        "\n",
        "    my_file = Path(join(Base_dir,\n",
        "        '2D-models/best-{}-{}-{}-{}-{}.h5'.format(epochs, seq_len, number_filter, drop, i)))\n",
        "    filepath = join(Base_dir, '2D-models/best-{}-{}-{}-{}-{}.h5'.format(epochs, seq_len, number_filter, drop, i))\n",
        "    if my_file.is_file():\n",
        "        print('loading model')\n",
        "\n",
        "    else:\n",
        "\n",
        "        print(' fitting model to target')\n",
        "        model = Sequential()\n",
        "        #\n",
        "        # layer 1\n",
        "        model.add(\n",
        "            Conv2D(number_filter[0], (1, number_feature), activation='relu', input_shape=(seq_len, number_feature, 1)))\n",
        "        # layer 2\n",
        "        model.add(Conv2D(number_filter[1], (3, 1), activation='relu'))\n",
        "        model.add(MaxPool2D(pool_size=(2, 1)))\n",
        "\n",
        "        # layer 3\n",
        "        model.add(Conv2D(number_filter[2], (3, 1), activation='relu'))\n",
        "        model.add(MaxPool2D(pool_size=(2, 1)))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dropout(drop))\n",
        "\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "        model.compile(optimizer='Adam', loss='mae', metrics=['acc', f1])\n",
        "\n",
        "        best_model = callbacks.ModelCheckpoint(filepath, monitor='val_f1', verbose=0, save_best_only=True,\n",
        "                                               save_weights_only=False, mode='max', period=1)\n",
        "\n",
        "\n",
        "        model.fit(cnn_train_data, cnn_train_target, epochs=epochs, batch_size=128, verbose=1,\n",
        "                        validation_data=(cnn_valid_data, cnn_valid_target), callbacks=[best_model])\n",
        "    model = load_model(filepath, custom_objects={'f1': f1})\n",
        "\n",
        "    return model, seq_len\n",
        "\n",
        "\n",
        "def cnn_data_sequence_pre_train(data, target, seque_len):\n",
        "    new_data = []\n",
        "    new_target = []\n",
        "    for index in range(data.shape[0] - seque_len + 1):\n",
        "        new_data.append(data[index: index + seque_len])\n",
        "        new_target.append(target[index + seque_len - 1])\n",
        "\n",
        "    new_data = np.array(new_data)\n",
        "    new_target = np.array(new_target)\n",
        "\n",
        "    new_data = new_data.reshape(new_data.shape[0], new_data.shape[1], new_data.shape[2], 1)\n",
        "\n",
        "    return new_data, new_target\n",
        "\n",
        "\n",
        "def prediction(data_warehouse, model, seque_len, order_stocks, cnn_results):\n",
        "    for name in order_stocks:\n",
        "        value = data_warehouse[name]\n",
        "        # train_data, train_target = cnn_data_sequence_pre_train(value[0], value[1], seque_len)\n",
        "        test_data, test_target = cnn_data_sequence_pre_train(value[2], value[3], seque_len)\n",
        "        # valid_data, valid_target = cnn_data_sequence_pre_train(value[4], value[5], seque_len)\n",
        "\n",
        "        cnn_results.append(sklearn_acc(model, test_data, test_target)[2])\n",
        "\n",
        "    return cnn_results\n",
        "\n",
        "\n",
        "def run_cnn_ann(data_warehouse, order_stocks):\n",
        "    cnn_results = []\n",
        "    # dnn_results = []\n",
        "    iterate_no = 4\n",
        "    for i in range(1, iterate_no):\n",
        "        K.clear_session()\n",
        "        print(i)\n",
        "        model, seq_len = train(data_warehouse, i)\n",
        "        # cnn_results, dnn_results = prediction(data_warehouse, model, seq_len, order_stocks, cnn_results)\n",
        "        cnn_results = prediction(data_warehouse, model, seq_len, order_stocks, cnn_results)\n",
        "\n",
        "    cnn_results = np.array(cnn_results)\n",
        "    cnn_results = cnn_results.reshape(iterate_no - 1, len(order_stocks))\n",
        "    cnn_results = pd.DataFrame(cnn_results, columns=order_stocks)\n",
        "    cnn_results = cnn_results.append([cnn_results.mean(), cnn_results.max(), cnn_results.std()], ignore_index=True)\n",
        "    cnn_results.to_csv(join(Base_dir, '2D-models/new results.csv'), index=False)\n",
        "\n",
        "\n",
        "Base_dir = ''\n",
        "TRAIN_ROOT_PATH = join(Base_dir, 'Dataset')\n",
        "train_file_names = os.listdir(join(Base_dir, 'Dataset'))\n",
        "\n",
        "# if moving average = 0 then we have no moving average\n",
        "seq_len = 60\n",
        "moving_average_day = 0\n",
        "number_of_stocks = 0\n",
        "number_feature = 0\n",
        "samples_in_each_stock = 0\n",
        "number_filter = [8, 8, 8]\n",
        "predict_day = 1\n",
        "\n",
        "cnn_train_data, cnn_train_target, cnn_test_data, cnn_test_target, cnn_valid_data, cnn_valid_target = ([] for i in\n",
        "                                                                                                      range(6))\n",
        "\n",
        "print('Loading train data ...')\n",
        "order_stocks = []\n",
        "data_warehouse = costruct_data_warehouse(TRAIN_ROOT_PATH, train_file_names)\n",
        "# order_stocks = data_warehouse.keys()\n",
        "\n",
        "print('number of stocks = '), number_of_stocks\n",
        "\n",
        "run_cnn_ann(data_warehouse, order_stocks)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mcZJhNSvsbsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simplified Implementation"
      ],
      "metadata": {
        "id": "PH17EKa6xSc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        " \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error"
      ],
      "metadata": {
        "id": "tizls2Rry9tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/mandalnilabja/soc2022/main/Processed_DJI.csv', index_col=\"Date\", parse_dates=True)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "cCf9OKiQx8XM",
        "outputId": "e79b57dd-8fd1-4ad6-c115-1bae910912d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Close    Volume       mom      mom1      mom2      mom3  \\\n",
              "Date                                                                         \n",
              "2009-12-31  10428.049805       NaN       NaN       NaN       NaN       NaN   \n",
              "2010-01-04  10583.959961       NaN  0.014951       NaN       NaN       NaN   \n",
              "2010-01-05  10572.019531       NaN -0.001128  0.014951       NaN       NaN   \n",
              "2010-01-06  10573.679688  0.515598  0.000157 -0.001128  0.014951       NaN   \n",
              "2010-01-07  10606.860352  9.776045  0.003138  0.000157 -0.001128  0.014951   \n",
              "\n",
              "            ROC_5  ROC_10  ROC_15  ROC_20  ...   NZD  silver-F  RUSSELL-F  \\\n",
              "Date                                       ...                              \n",
              "2009-12-31    NaN     NaN     NaN     NaN  ...  0.03      0.26      -1.08   \n",
              "2010-01-04    NaN     NaN     NaN     NaN  ...  1.52      3.26       1.61   \n",
              "2010-01-05    NaN     NaN     NaN     NaN  ... -0.07      1.96      -0.20   \n",
              "2010-01-06    NaN     NaN     NaN     NaN  ...  0.56      2.15      -0.02   \n",
              "2010-01-07    NaN     NaN     NaN     NaN  ... -0.72      0.94       0.50   \n",
              "\n",
              "            S&P-F   CHF  Dollar index-F  Dollar index  wheat-F   XAG   XAU  \n",
              "Date                                                                        \n",
              "2009-12-31  -1.00 -0.11           -0.08         -0.06    -0.48  0.30  0.39  \n",
              "2010-01-04   1.62 -0.57           -0.59         -0.42     3.12  3.91  2.10  \n",
              "2010-01-05   0.31  0.43            0.03          0.12    -0.90  1.42 -0.12  \n",
              "2010-01-06   0.07 -0.56           -0.24         -0.17     2.62  2.25  1.77  \n",
              "2010-01-07   0.40  0.58            0.58          0.54    -1.85  0.22 -0.58  \n",
              "\n",
              "[5 rows x 83 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9b9a4595-2b61-4fda-a3a4-95b397d87aaa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>mom</th>\n",
              "      <th>mom1</th>\n",
              "      <th>mom2</th>\n",
              "      <th>mom3</th>\n",
              "      <th>ROC_5</th>\n",
              "      <th>ROC_10</th>\n",
              "      <th>ROC_15</th>\n",
              "      <th>ROC_20</th>\n",
              "      <th>...</th>\n",
              "      <th>NZD</th>\n",
              "      <th>silver-F</th>\n",
              "      <th>RUSSELL-F</th>\n",
              "      <th>S&amp;P-F</th>\n",
              "      <th>CHF</th>\n",
              "      <th>Dollar index-F</th>\n",
              "      <th>Dollar index</th>\n",
              "      <th>wheat-F</th>\n",
              "      <th>XAG</th>\n",
              "      <th>XAU</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2009-12-31</th>\n",
              "      <td>10428.049805</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.26</td>\n",
              "      <td>-1.08</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>-0.48</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-04</th>\n",
              "      <td>10583.959961</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.014951</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>1.52</td>\n",
              "      <td>3.26</td>\n",
              "      <td>1.61</td>\n",
              "      <td>1.62</td>\n",
              "      <td>-0.57</td>\n",
              "      <td>-0.59</td>\n",
              "      <td>-0.42</td>\n",
              "      <td>3.12</td>\n",
              "      <td>3.91</td>\n",
              "      <td>2.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-05</th>\n",
              "      <td>10572.019531</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.001128</td>\n",
              "      <td>0.014951</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>1.96</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.12</td>\n",
              "      <td>-0.90</td>\n",
              "      <td>1.42</td>\n",
              "      <td>-0.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-06</th>\n",
              "      <td>10573.679688</td>\n",
              "      <td>0.515598</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>-0.001128</td>\n",
              "      <td>0.014951</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.56</td>\n",
              "      <td>2.15</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.07</td>\n",
              "      <td>-0.56</td>\n",
              "      <td>-0.24</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>2.62</td>\n",
              "      <td>2.25</td>\n",
              "      <td>1.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-07</th>\n",
              "      <td>10606.860352</td>\n",
              "      <td>9.776045</td>\n",
              "      <td>0.003138</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>-0.001128</td>\n",
              "      <td>0.014951</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.72</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.54</td>\n",
              "      <td>-1.85</td>\n",
              "      <td>0.22</td>\n",
              "      <td>-0.58</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 83 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9b9a4595-2b61-4fda-a3a4-95b397d87aaa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9b9a4595-2b61-4fda-a3a4-95b397d87aaa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9b9a4595-2b61-4fda-a3a4-95b397d87aaa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir={'DJI':'https://raw.githubusercontent.com/mandalnilabja/soc2022/main/Processed_DJI.csv',\n",
        "     'NASDAQ':'https://raw.githubusercontent.com/mandalnilabja/soc2022/main/Processed_NASDAQ.csv',\n",
        "     'NYSE':'https://raw.githubusercontent.com/mandalnilabja/soc2022/main/Processed_NYSE.csv',\n",
        "     'RUSSELL':'https://raw.githubusercontent.com/mandalnilabja/soc2022/main/Processed_RUSSELL.csv',\n",
        "     'S&P':'https://raw.githubusercontent.com/mandalnilabja/soc2022/main/Processed_S&P.csv'\n",
        "     }\n",
        "\n",
        "\n",
        "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
        "TRAIN_VALID_RATIO = 0.75\n",
        "\n",
        "\n",
        "data = {}\n",
        "\n",
        "\n",
        "for name, filepath in dir.items():\n",
        "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
        "    # basic preprocessing: get the name, the classification\n",
        "    # Save the target variable as a column in dataframe for easier dropna()\n",
        "    del X[\"Name\"]\n",
        "    cols = X.columns\n",
        "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
        "    X.dropna(inplace=True)\n",
        "    # Fit the standard scaler using the training dataset\n",
        "    index = X.index[X.index > TRAIN_TEST_CUTOFF]\n",
        "    index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
        "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
        "    # Save scale transformed dataframe\n",
        "    X[cols] = scaler.transform(X[cols])\n",
        "    data[name] = X"
      ],
      "metadata": {
        "id": "aJowCJds2VcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        " \n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        " \n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        " \n",
        "def f1macro(y_true, y_pred):\n",
        "    f_pos = f1_m(y_true, y_pred)\n",
        "    # negative version of the data and prediction\n",
        "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
        "    return (f_pos + f_neg)/2"
      ],
      "metadata": {
        "id": "ify4vvFeHG7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cnnpred_2d(seq_len=60, n_features=82, n_filters=(8,8,8), droprate=0.1):\n",
        "    \"2D-CNNpred model according to the paper\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(seq_len, n_features, 1)),\n",
        "        Conv2D(n_filters[0], kernel_size=(1, n_features), activation=\"relu\"),\n",
        "        Conv2D(n_filters[1], kernel_size=(3,1), activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,1)),\n",
        "        Conv2D(n_filters[2], kernel_size=(3,1), activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,1)),\n",
        "        Flatten(),\n",
        "        Dropout(droprate),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    return model\n",
        " "
      ],
      "metadata": {
        "id": "wl7EiqwoHJFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def datagen(data, seq_len, batch_size, targetcol, kind):\n",
        "    \"As a generator to produce samples for Keras model\"\n",
        "    batch = []\n",
        "    while True:\n",
        "        # Pick one dataframe from the pool\n",
        "        key = random.choice(list(data.keys()))\n",
        "        df = data[key]\n",
        "        input_cols = [c for c in df.columns if c != targetcol]\n",
        "        index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
        "        split = int(len(index) * TRAIN_VALID_RATIO)\n",
        "        if kind == 'train':\n",
        "            index = index[:split]   # range for the training set\n",
        "        elif kind == 'valid':\n",
        "            index = index[split:]   # range for the validation set\n",
        "        # Pick one position, then clip a sequence length\n",
        "        while True:\n",
        "            t = random.choice(index)      # pick one time step\n",
        "            n = (df.index == t).argmax()  # find its position in the dataframe\n",
        "            if n-seq_len+1 < 0:\n",
        "                continue # can't get enough data for one sequence length\n",
        "            frame = df.iloc[n-seq_len+1:n+1]\n",
        "            batch.append([frame[input_cols].values, df.loc[t, targetcol]])\n",
        "            break\n",
        "        # if we get enough for a batch, dispatch\n",
        "        if len(batch) == batch_size:\n",
        "            X, y = zip(*batch)\n",
        "            X, y = np.expand_dims(np.array(X), 3), np.array(y)\n",
        "            yield X, y\n",
        "            batch = []"
      ],
      "metadata": {
        "id": "ldA0d-fC166S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testgen(data, seq_len, targetcol):\n",
        "    \"Return array of all test samples\"\n",
        "    batch = []\n",
        "    for key, df in data.items():\n",
        "        input_cols = [c for c in df.columns if c != targetcol]\n",
        "        # find the start of test sample\n",
        "        t = df.index[df.index >= TRAIN_TEST_CUTOFF][0]\n",
        "        n = (df.index == t).argmax()\n",
        "        # extract sample using a sliding window\n",
        "        for i in range(n+1, len(df)+1):\n",
        "            frame = df.iloc[i-seq_len:i]\n",
        "            batch.append([frame[input_cols].values, frame[targetcol][-1]])\n",
        "    X, y = zip(*batch)\n",
        "    return np.expand_dims(np.array(X),3), np.array(y)"
      ],
      "metadata": {
        "id": "jif16IhIHLAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 60\n",
        "batch_size = 128\n",
        "n_epochs = 20\n",
        "n_features = 82\n",
        " \n",
        "# Produce CNNpred as a binary classification problem\n",
        "model = cnnpred_2d(seq_len, n_features)\n",
        "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"acc\", f1macro])\n",
        "model.summary()  # print model structure to console\n",
        " \n",
        "# Set up callbacks and fit the model\n",
        "# We use custom validation score f1macro() and hence monitor for \"val_f1macro\"\n",
        "checkpoint_path = \"./cp2d-{epoch}-{val_f1macro:.2f}.h5\"\n",
        "callbacks = [\n",
        "    ModelCheckpoint(checkpoint_path,\n",
        "                    monitor='val_f1macro', mode=\"max\",\n",
        "                    verbose=0, save_best_only=True, save_weights_only=False, save_freq=\"epoch\")\n",
        "]\n",
        "model.fit(datagen(data, seq_len, batch_size, \"Target\", \"train\"),\n",
        "          validation_data=datagen(data, seq_len, batch_size, \"Target\", \"valid\"),\n",
        "          epochs=n_epochs, steps_per_epoch=400, validation_steps=10, verbose=1, callbacks=callbacks)\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImWlWrylHVwv",
        "outputId": "3f0589b3-013d-484b-a513-81f54f055629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 60, 1, 8)          664       \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 58, 1, 8)          200       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 29, 1, 8)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 27, 1, 8)          200       \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 13, 1, 8)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 104)               0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 104)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 105       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,169\n",
            "Trainable params: 1,169\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "400/400 [==============================] - 53s 130ms/step - loss: 0.4394 - acc: 0.5616 - f1macro: 0.3591 - val_loss: 0.4790 - val_acc: 0.5211 - val_f1macro: 0.3422\n",
            "Epoch 2/20\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.4430 - acc: 0.5570 - f1macro: 0.3573 - val_loss: 0.4774 - val_acc: 0.5227 - val_f1macro: 0.3431\n",
            "Epoch 3/20\n",
            "400/400 [==============================] - 50s 126ms/step - loss: 0.4377 - acc: 0.5623 - f1macro: 0.3595 - val_loss: 0.4914 - val_acc: 0.5086 - val_f1macro: 0.3368\n",
            "Epoch 4/20\n",
            "400/400 [==============================] - 51s 128ms/step - loss: 0.4402 - acc: 0.5598 - f1macro: 0.3584 - val_loss: 0.5141 - val_acc: 0.4859 - val_f1macro: 0.3266\n",
            "Epoch 5/20\n",
            "400/400 [==============================] - 52s 131ms/step - loss: 0.4394 - acc: 0.5606 - f1macro: 0.3588 - val_loss: 0.5102 - val_acc: 0.4898 - val_f1macro: 0.3286\n",
            "Epoch 6/20\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.4378 - acc: 0.5622 - f1macro: 0.3593 - val_loss: 0.4891 - val_acc: 0.5109 - val_f1macro: 0.3373\n",
            "Epoch 7/20\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.4344 - acc: 0.5664 - f1macro: 0.3814 - val_loss: 0.5104 - val_acc: 0.5031 - val_f1macro: 0.4769\n",
            "Epoch 8/20\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.3058 - acc: 0.7062 - f1macro: 0.6716 - val_loss: 0.5158 - val_acc: 0.4977 - val_f1macro: 0.4758\n",
            "Epoch 9/20\n",
            "400/400 [==============================] - 50s 126ms/step - loss: 0.2442 - acc: 0.7687 - f1macro: 0.7537 - val_loss: 0.5473 - val_acc: 0.4531 - val_f1macro: 0.4337\n",
            "Epoch 10/20\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.2214 - acc: 0.7886 - f1macro: 0.7761 - val_loss: 0.5300 - val_acc: 0.4633 - val_f1macro: 0.4342\n",
            "Epoch 11/20\n",
            "400/400 [==============================] - 52s 131ms/step - loss: 0.2055 - acc: 0.8036 - f1macro: 0.7936 - val_loss: 0.5415 - val_acc: 0.4437 - val_f1macro: 0.4072\n",
            "Epoch 12/20\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.1962 - acc: 0.8114 - f1macro: 0.8020 - val_loss: 0.5191 - val_acc: 0.4703 - val_f1macro: 0.4438\n",
            "Epoch 13/20\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.1877 - acc: 0.8195 - f1macro: 0.8111 - val_loss: 0.5188 - val_acc: 0.4805 - val_f1macro: 0.4540\n",
            "Epoch 14/20\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.1797 - acc: 0.8264 - f1macro: 0.8186 - val_loss: 0.4947 - val_acc: 0.5094 - val_f1macro: 0.4980\n",
            "Epoch 15/20\n",
            "400/400 [==============================] - 52s 129ms/step - loss: 0.1785 - acc: 0.8272 - f1macro: 0.8199 - val_loss: 0.4866 - val_acc: 0.5125 - val_f1macro: 0.4916\n",
            "Epoch 16/20\n",
            "400/400 [==============================] - 51s 127ms/step - loss: 0.1743 - acc: 0.8311 - f1macro: 0.8235 - val_loss: 0.5035 - val_acc: 0.4953 - val_f1macro: 0.4813\n",
            "Epoch 17/20\n",
            "400/400 [==============================] - 52s 130ms/step - loss: 0.1739 - acc: 0.8312 - f1macro: 0.8246 - val_loss: 0.5146 - val_acc: 0.4883 - val_f1macro: 0.4699\n",
            "Epoch 18/20\n",
            "400/400 [==============================] - 50s 126ms/step - loss: 0.1697 - acc: 0.8352 - f1macro: 0.8285 - val_loss: 0.5051 - val_acc: 0.4938 - val_f1macro: 0.4698\n",
            "Epoch 19/20\n",
            "400/400 [==============================] - 51s 129ms/step - loss: 0.1652 - acc: 0.8396 - f1macro: 0.8329 - val_loss: 0.4835 - val_acc: 0.5242 - val_f1macro: 0.5075\n",
            "Epoch 20/20\n",
            "400/400 [==============================] - 50s 125ms/step - loss: 0.1624 - acc: 0.8423 - f1macro: 0.8359 - val_loss: 0.4618 - val_acc: 0.5359 - val_f1macro: 0.5215\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3060351090>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "66s for one epoch with only CPU\n",
        "\n",
        "64s for one epoch with GPU\n",
        "\n",
        "17m for 20 epochs"
      ],
      "metadata": {
        "id": "GcFbH5y7H9ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare test data\n",
        "test_data, test_target = testgen(data, seq_len, \"Target\")\n",
        " \n",
        "# Test the model\n",
        "test_out = model.predict(test_data)\n",
        "test_pred = (test_out > 0.5).astype(int)\n",
        "print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
        "print(\"MAE:\", mean_absolute_error(test_pred, test_target))\n",
        "print(\"F1:\", f1_score(test_pred, test_target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDrHIgYPHgN_",
        "outputId": "1701096c-c5ae-46e6-9856-0a571b72a51f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.5034146341463415\n",
            "MAE: 0.49658536585365853\n",
            "F1: 0.5740585774058578\n"
          ]
        }
      ]
    }
  ]
}